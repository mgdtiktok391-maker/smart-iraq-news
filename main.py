# -*- coding: utf-8 -*-

import os
import random
import re
import requests
import feedparser
import backoff
import markdown as md

from google.oauth2.credentials import Credentials
from googleapiclient.discovery import build
from google.auth.transport.requests import Request

# =================================================
# ğŸ” Secrets (Ù…Ø·Ø§Ø¨Ù‚Ø© Ù„Ù„Ù€ workflow)
# =================================================

HF_API_KEY = os.getenv("HF_API_KEY")
CLIENT_ID = os.getenv("CLIENT_ID")
CLIENT_SECRET = os.getenv("CLIENT_SECRET")
REFRESH_TOKEN = os.getenv("REFRESH_TOKEN")
BLOG_URL = os.getenv("BLOG_URL")

required = {
    "HF_API_KEY": HF_API_KEY,
    "CLIENT_ID": CLIENT_ID,
    "CLIENT_SECRET": CLIENT_SECRET,
    "REFRESH_TOKEN": REFRESH_TOKEN,
}

missing = [k for k, v in required.items() if not v]
if missing:
    raise RuntimeError(f"âŒ Missing secrets: {', '.join(missing)}")

# =================================================
# ğŸ“° Blogger API
# =================================================

def get_blogger_service():
    creds = Credentials(
        token=None,
        refresh_token=REFRESH_TOKEN,
        token_uri="https://oauth2.googleapis.com/token",
        client_id=CLIENT_ID,
        client_secret=CLIENT_SECRET,
        scopes=["https://www.googleapis.com/auth/blogger"],
    )
    creds.refresh(Request())
    return build("blogger", "v3", credentials=creds, cache_discovery=False)

def get_blog_id(service):
    blogs = service.blogs().listByUser(userId="self").execute()
    if blogs.get("items"):
        b = blogs["items"][0]
        return b["id"], b["name"]
    if BLOG_URL:
        b = service.blogs().getByUrl(url=BLOG_URL).execute()
        return b["id"], b["name"]
    return None, None

def get_recent_titles(service, blog_id):
    posts = service.posts().list(
        blogId=blog_id,
        fetchBodies=False,
        maxResults=15
    ).execute()
    return [p.get("title", "") for p in posts.get("items", [])]

# =================================================
# ğŸ§  Topics
# =================================================

FALLBACK_TOPICS = [
    "Ø£ÙØ¶Ù„ Ø·Ø±Ù‚ Ø­Ù…Ø§ÙŠØ© Ø§Ù„Ø®ØµÙˆØµÙŠØ© Ø¹Ù„Ù‰ Ø§Ù„Ø¥Ù†ØªØ±Ù†Øª",
    "ÙƒÙŠÙ ØªØ­Ù…ÙŠ Ø¨ÙŠØ§Ù†Ø§ØªÙƒ Ø§Ù„Ø´Ø®ØµÙŠØ© Ù…Ù† Ø§Ù„Ø§Ø®ØªØ±Ø§Ù‚",
    "Ø£Ù‡Ù… Ø£Ø¯ÙˆØ§Øª Ø§Ù„Ø¥Ù†ØªØ§Ø¬ÙŠØ© Ø§Ù„Ø±Ù‚Ù…ÙŠØ© ÙÙŠ 2025",
    "Ù…Ø³ØªÙ‚Ø¨Ù„ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ ÙÙŠ Ø§Ù„ØªØ¹Ù„ÙŠÙ…",
    "ÙƒÙŠÙ ÙŠØ¤Ø«Ø± Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø¹Ù„Ù‰ Ø³ÙˆÙ‚ Ø§Ù„Ø¹Ù…Ù„",
    "Ø¯Ù„ÙŠÙ„ Ø§Ù„Ù…Ø¨ØªØ¯Ø¦ÙŠÙ† Ø¥Ù„Ù‰ Ø§Ù„Ø£Ù…Ù† Ø§Ù„Ø³ÙŠØ¨Ø±Ø§Ù†ÙŠ",
    "Ø£Ø®Ø·Ø± Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ Ø§Ù„Ø´Ø§Ø¦Ø¹Ø© ÙÙŠ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¥Ù†ØªØ±Ù†Øª",
    "ÙƒÙŠÙ ØªØ®ØªØ§Ø± ÙƒÙ„Ù…Ø© Ù…Ø±ÙˆØ± Ù‚ÙˆÙŠØ© ÙˆØ¢Ù…Ù†Ø©",
    "Ø§Ù„ÙØ±Ù‚ Ø¨ÙŠÙ† Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ ÙˆØ§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ",
    "Ø£Ù‡Ù… ØªØ·Ø¨ÙŠÙ‚Ø§Øª Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ ÙÙŠ Ø§Ù„Ø­ÙŠØ§Ø© Ø§Ù„ÙŠÙˆÙ…ÙŠØ©",
    "ÙƒÙŠÙ ÙŠØ¹Ù…Ù„ Ø§Ù„Ø¥Ù†ØªØ±Ù†Øª Ù…Ù† Ø§Ù„Ù†Ø§Ø­ÙŠØ© Ø§Ù„ØªÙ‚Ù†ÙŠØ©",
    "Ù…ÙÙ‡ÙˆÙ… Ø§Ù„Ø­ÙˆØ³Ø¨Ø© Ø§Ù„Ø³Ø­Ø§Ø¨ÙŠØ© Ø¨Ø·Ø±ÙŠÙ‚Ø© Ù…Ø¨Ø³Ø·Ø©",
    "Ø¥ÙŠØ¬Ø§Ø¨ÙŠØ§Øª ÙˆØ³Ù„Ø¨ÙŠØ§Øª Ø§Ù„Ø¹Ù…Ù„ Ø¹Ù† Ø¨ÙØ¹Ø¯",
    "ÙƒÙŠÙ ØªØ¨Ø¯Ø£ Ø§Ù„Ø¹Ù…Ù„ Ø§Ù„Ø­Ø± Ø®Ø·ÙˆØ© Ø¨Ø®Ø·ÙˆØ©",
    "Ø£ÙØ¶Ù„ Ø§Ù„Ù…Ù‡Ø§Ø±Ø§Øª Ø§Ù„Ø±Ù‚Ù…ÙŠØ© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© ÙÙŠ Ø§Ù„Ù…Ø³ØªÙ‚Ø¨Ù„",
    "Ø´Ø±Ø­ ØªÙ‚Ù†ÙŠØ© Ø§Ù„Ø¨Ù„ÙˆÙƒ ØªØ´ÙŠÙ† Ù„Ù„Ù…Ø¨ØªØ¯Ø¦ÙŠÙ†",
    "Ù…Ø§ Ù‡Ùˆ Ø¥Ù†ØªØ±Ù†Øª Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ ÙˆÙƒÙŠÙ ÙŠØ¹Ù…Ù„",
    "ÙƒÙŠÙ ØªÙ…ÙŠÙ‘Ø² Ø¨ÙŠÙ† Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ø§Ù„ØµØ­ÙŠØ­Ø© ÙˆØ§Ù„Ù…Ø¶Ù„Ù„Ø©",
    "Ù…Ø³ØªÙ‚Ø¨Ù„ Ø§Ù„ØªØ¬Ø§Ø±Ø© Ø§Ù„Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠØ© Ø¹Ø§Ù„Ù…ÙŠÙ‹Ø§",
    "Ø£Ù‡Ù…ÙŠØ© Ø§Ù„ØªÙÙƒÙŠØ± Ø§Ù„Ù†Ù‚Ø¯ÙŠ ÙÙŠ Ø§Ù„Ø¹ØµØ± Ø§Ù„Ø±Ù‚Ù…ÙŠ",
]

def clean(text):
    return re.sub(r"[^\w\s]", "", text).lower()

def is_duplicate(title, history):
    nw = set(clean(title).split())
    for h in history:
        ow = set(clean(h).split())
        if nw and len(nw & ow) / len(nw) > 0.5:
            return True
    return False

def get_trends():
    urls = [
        "https://trends.google.com/trends/trendingsearches/daily/rss?geo=SA",
        "https://trends.google.com/trends/trendingsearches/daily/rss?geo=EG",
    ]
    topics = []
    for url in urls:
        feed = feedparser.parse(url)
        for e in feed.entries[:2]:
            topics.append(e.title)
    topics.extend(FALLBACK_TOPICS)
    random.shuffle(topics)
    return topics

# =================================================
# ğŸ¤– Hugging Face (Ù†Ù…ÙˆØ°Ø¬ Ù…Ø³ØªÙ‚Ø±)
# =================================================

@backoff.on_exception(backoff.expo, Exception, max_tries=3)
def generate_article(topic):
    print(f"âœ Writing article: {topic}")

    url = "https://api-inference.huggingface.co/models/HuggingFaceH4/zephyr-7b-beta"
    headers = {
        "Authorization": f"Bearer {HF_API_KEY}",
        "Content-Type": "application/json",
    }

    prompt = f"""
Ø§ÙƒØªØ¨ Ù…Ù‚Ø§Ù„Ù‹Ø§ ØªÙ‚Ù†ÙŠÙ‹Ø§ Ø¹Ø±Ø¨ÙŠÙ‹Ø§ Ø§Ø­ØªØ±Ø§ÙÙŠÙ‹Ø§ Ø¨Ø¹Ù†ÙˆØ§Ù†:
{topic}

Ø§Ù„Ø´Ø±ÙˆØ·:
- Ù„ØºØ© Ø¹Ø±Ø¨ÙŠØ© ÙØµØ­Ù‰
- ØªÙ†Ø³ÙŠÙ‚ Markdown
- Ù„Ø§ ÙŠÙ‚Ù„ Ø¹Ù† 500 ÙƒÙ„Ù…Ø©
- Ø¨Ø¯ÙˆÙ† Ù…Ù‚Ø¯Ù…Ø§Øª Ø²Ø§Ø¦Ø¯Ø©
"""

    payload = {
        "inputs": prompt,
        "parameters": {
            "temperature": 0.7,
            "max_new_tokens": 1200,
            "return_full_text": False
        }
    }

    r = requests.post(url, headers=headers, json=payload, timeout=180)
    if r.status_code != 200:
        raise RuntimeError(f"HF error {r.status_code}: {r.text}")

    data = r.json()
    if isinstance(data, list):
        return data[0]["generated_text"]
    raise RuntimeError(f"Unexpected HF response: {data}")

def get_image():
    seed = random.randint(1, 9999)
    return (
        "https://image.pollinations.ai/prompt/"
        "futuristic%20technology%20background"
        f"?width=800&height=450&seed={seed}&nologo=true"
    )

# =================================================
# ğŸš€ Main
# =================================================

def main():
    print("ğŸš€ Smart Iraq News Bot started")

    service = get_blogger_service()
    blog_id, blog_name = get_blog_id(service)
    if not blog_id:
        print("âŒ No blog found")
        return

    print(f"âœ… Connected to blog: {blog_name}")

    history = get_recent_titles(service, blog_id)
    topic = next(
        (t for t in get_trends() if not is_duplicate(t, history)),
        random.choice(FALLBACK_TOPICS)
    )

    print(f"ğŸ“ Selected topic: {topic}")

    md_text = generate_article(topic)

    lines = md_text.strip().split("\n")
    title = topic
    if lines and lines[0].startswith("#"):
        title = lines[0].replace("#", "").strip()
        md_text = "\n".join(lines[1:])

    html = md.markdown(md_text)
    img = get_image()

    body = {
        "title": title,
        "content": f"""
<div style="text-align:center;margin-bottom:20px">
<img src="{img}" style="max-width:100%;border-radius:12px">
</div>
<div dir="rtl" style="text-align:right;line-height:1.8">
{html}
</div>
"""
    }

    post = service.posts().insert(
        blogId=blog_id,
        body=body,
        isDraft=False
    ).execute()

    print(f"ğŸ‰ Published: {post.get('url')}")

if __name__ == "__main__":
    main()
